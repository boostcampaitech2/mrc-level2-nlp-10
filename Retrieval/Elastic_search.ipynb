{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Elasticsearch in /opt/conda/lib/python3.8/site-packages (7.15.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from Elasticsearch) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from Elasticsearch) (2021.10.8)\n",
      " * Starting Elasticsearch Server\n",
      " * Already running.\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# Install ElasticSearch\n",
    "!pip install Elasticsearch\n",
    "!service elasticsearch start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elastic_retriever():\n",
    "\n",
    "    def __init__(self, wiki_json_path, hundred = False, index_name = None):\n",
    "        self.wiki_list = [] # saves text and document id only\n",
    "        \n",
    "        # load wikipedia data and drop needless informations \n",
    "        with open(wiki_json_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "            if hundred:\n",
    "                len_wiki = 100\n",
    "            else:\n",
    "                len_wiki = len(wiki)\n",
    "\n",
    "            for ind in range(len_wiki):\n",
    "                temp_wiki = wiki[str(ind)]\n",
    "                self.wiki_list.append({\"text\": temp_wiki[\"text\"], \"document_id\" : temp_wiki[\"document_id\"]})\n",
    "\n",
    "        del wiki # for memory usage\n",
    "        self.es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "        if index_name is not None:\n",
    "            self.index_name = index_name\n",
    "        else:\n",
    "            self.index_name = 'klue_mrc_wikipedia_index'\n",
    "\n",
    "    def _create_indice(self, index_config = None):\n",
    "\n",
    "        if index_config is None:\n",
    "            index_config = {\n",
    "                \"settings\": {\n",
    "                    \"analysis\": {\n",
    "                        \"analyzer\": {\n",
    "                            \"standard_analyzer\": {\n",
    "                                \"type\": \"standard\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"mappings\": {\n",
    "                    \"dynamic\": \"strict\", \n",
    "                    \"properties\": {\n",
    "                        \"document_id\": {\"type\": \"long\",},\n",
    "                        \"text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        if self.es.indices.exists(index=self.index_name):\n",
    "            self.es.indices.delete(index=self.index_name)\n",
    "\n",
    "        self.es.indices.create(index=self.index_name, body=index_config, ignore=400)\n",
    "\n",
    "    def _populate_index(self):\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(self.wiki_list))):\n",
    "            self.es.index(index = self.index_name, id = i, body = self.wiki_list[i])\n",
    "        \n",
    "    def config_and_index(self, index_name = None, index_config = None):\n",
    "        self._create_indice(index_name, index_config)\n",
    "        self._populate_index(index_name)\n",
    "\n",
    "    def search(self, query, num_return, index_name = None):\n",
    "        if index_name is None:\n",
    "            index_name = 'klue_mrc_wikipedia_index'\n",
    "        answer = self.es.search(index=index_name, q = query, size = num_return)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "<ipython-input-3-c7d0f08d5220>:52: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  self.es.indices.create(index=self.index_name, body=index_config, ignore=400)\n",
      "  0%|          | 0/60613 [00:00<?, ?it/s]<ipython-input-3-c7d0f08d5220>:57: DeprecationWarning: The 'body' parameter is deprecated for the 'index' API and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  self.es.index(index = self.index_name, id = i, body = self.wiki_list[i])\n",
      "100%|██████████| 60613/60613 [04:36<00:00, 219.18it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_json_path = \"/opt/ml/code/preprocessed_json_v3.json\"\n",
    "\n",
    "retriever = Elastic_retriever(wiki_json_path)\n",
    "retriever._create_indice(index_config = index_config)\n",
    "retriever._populate_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\":{\n",
    "                    \"my_stop_filter\": { \n",
    "                        \"type\" : \"stop\",\n",
    "                        \"stopwords_path\" : \"stop_words.txt\" # /etc/elastic안에 txt파일이 존재해야 댑니다\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"nori_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"nori_tokenizer\", # 노리 형태소 깔아야대는데 에러나면 맨위에 참고해서 깔기\n",
    "                        \"decompound_mode\": \"discard\",\n",
    "                        \"filter\" : [\"my_stop_filter\"]# 위에서 정의한 stopword\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"dynamic\": \"strict\", # 먼지 잘모르겟\n",
    "            \"properties\": {\n",
    "                \"document_id\": {\"type\": \"long\",},\n",
    "                \"text\": {\"type\": \"text\", \"analyzer\": \"nori_analyzer\"}\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(train_path, valid_path):\n",
    "    # make a list of dictionary\n",
    "    total_data = [] # {context, question, document_id}\n",
    "\n",
    "    train_df = pd.read_csv(train_path, index_col = 0)\n",
    "    valid_df = pd.read_csv(valid_path, index_col = 0)\n",
    "\n",
    "    total_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "    for i in range(len(total_df)):\n",
    "        temp_data = total_df.iloc[i]\n",
    "        total_data.append({\"text\":temp_data.context, \"question\":temp_data.question, \"document_id\": temp_data.document_id})\n",
    "\n",
    "    return total_data\n",
    "\n",
    "train_data = \"/opt/ml/code/train_dataset_no_tilde.csv\"\n",
    "valid_data = \"/opt/ml/code/valid_dataset_no_tilde.csv\"\n",
    "\n",
    "total_data = convert(train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import re\n",
    "\n",
    "def show_the_result(retriever, total_data):\n",
    "\n",
    "    results = [0]*21\n",
    "\n",
    "    total_data_len = len(total_data)\n",
    "    \n",
    "    for ind in tqdm.tqdm(range(total_data_len)):\n",
    "\n",
    "        temp_data = total_data[ind]\n",
    "        query = re.sub(\"~\",\"-\", temp_data[\"question\"])\n",
    "        query = re.sub(\"/\",\"\", query)\n",
    "        document_id = temp_data[\"document_id\"]\n",
    "\n",
    "        hit_ones = retriever.search(query, 20)[\"hits\"][\"hits\"]\n",
    "\n",
    "        if hit_ones: #만약 검출이 되었다면\n",
    "            result = [hit_one[\"_source\"][\"document_id\"] for hit_one in hit_ones]\n",
    "            \n",
    "            if document_id in result:\n",
    "                found_index = result.index(document_id)\n",
    "                results[found_index] +=1 \n",
    "            else:\n",
    "                results[-1] += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4192/4192 [00:47<00:00, 88.25it/s]\n"
     ]
    }
   ],
   "source": [
    "results = show_the_result(retriever, total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_result(result):\n",
    "    total = sum(result)\n",
    "    top_1 = result[0]\n",
    "    top_5 = sum(result[:2])\n",
    "    top_10 = sum(result[:3])\n",
    "    top_20 = sum(result[:4])\n",
    "\n",
    "    print(f\"===Retrieval Result===\\n\")\n",
    "    print(f\"top 1 : {top_1*100/total}%\")\n",
    "    print(f\"top 5 : {top_5*100/total}%\")\n",
    "    print(f\"top 10 : {top_10*100/total}%\")\n",
    "    print(f\"top 20 : {top_20*100/total}%\")\n",
    "    print(f\"failed to predict : {result[-1]*100/total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Retrieval Result===\n",
      "\n",
      "top 1 : 54.87950369840134%\n",
      "top 5 : 85.65974707706991%\n",
      "top 10 : 88.69005010737294%\n",
      "top 20 : 91.86351706036746%\n",
      "failed to predict : 8.136482939632545%\n"
     ]
    }
   ],
   "source": [
    "pretty_result(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
